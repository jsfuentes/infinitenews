{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Forced Alignment with Wav2Vec2\n",
    "\n",
    "Running Force Alignment with PyTorch wav2vec2 models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The process of alignment looks like the following.\n",
    "\n",
    "1. Estimate the frame-wise label probability from audio waveform\n",
    "2. Generate the trellis matrix which represents the probability of\n",
    "   labels aligned at time step.\n",
    "3. Find the most likely path from the trellis matrix.\n",
    "\n",
    "In this example, we use ``torchaudio``\\ â€™s ``Wav2Vec2`` model for\n",
    "acoustic feature extraction.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "First we import the necessary packages, and fetch data that we work on.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams[\"figure.figsize\"] = [16.0, 4.8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate frame-wise label probability\n",
    "\n",
    "The first step is to generate the label class porbability of each aduio\n",
    "frame. We can use a Wav2Vec2 model that is trained for ASR. Here we use\n",
    ":py:func:`torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H`.\n",
    "\n",
    "``torchaudio`` provides easy access to pretrained models with associated\n",
    "labels.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>In the subsequent sections, we will compute the probability in\n",
    "   log-domain to avoid numerical instability. For this purpose, we\n",
    "   normalize the ``emission`` with :py:func:`torch.log_softmax`.</p></div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to input speech file\n",
    "SPEECH_FILE = '/home/paperspace/repos/transformers/examples/research_projects/wav2vec2/wavs/g2.wav'\n",
    "\n",
    "# transcript for this file\n",
    "TEXT_FILE = '/home/paperspace/repos/transformers/examples/research_projects/wav2vec2/transcript_clean.txt'\n",
    "\n",
    "\n",
    "# model we are loading\n",
    "MODEL_NAME = 'WAV2VEC2_ASR_LARGE_LV60K_960H'\n",
    "# sampling rate the model expects\n",
    "# NOTE: most wav2vec models assume 16k sampling rate\n",
    "MODEL_SR = 16_000\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Point:\n",
    "    token_index: int\n",
    "    time_index: int\n",
    "    score: float\n",
    "\n",
    "\n",
    "# Merge the labels\n",
    "@dataclass\n",
    "class Segment:\n",
    "    label: str\n",
    "    start: int\n",
    "    end: int\n",
    "    score: float\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.label}\\t({self.score:4.2f}): [{self.start:5d}, {self.end:5d})\"\n",
    "\n",
    "    @property\n",
    "    def length(self):\n",
    "        return self.end - self.start\n",
    "\n",
    "\n",
    "\n",
    "def load_audio(wav_file, model_sr=MODEL_SR):\n",
    "    '''Loads `wav_file` using torchaudio.\n",
    "    \n",
    "    Resamples the audio to `MODEL_SR` if needed.\n",
    "    '''\n",
    "    wav, sr = torchaudio.load(wav_file)\n",
    "    if sr != model_sr:\n",
    "        print(f'Resampling from {sr} to {MODEL_SR}')\n",
    "        wav = torchaudio.transforms.Resample(sr, MODEL_SR)(wav)\n",
    "    return wav\n",
    "\n",
    "\n",
    "def clean_text(o):\n",
    "    clean = o.replace(',', ' ')\n",
    "    clean = clean.strip('\\n')\n",
    "    return clean\n",
    "\n",
    "def load_transcript(text_file):\n",
    "    '''Loads a transcript in `text_file`.\n",
    "    \n",
    "    Assumes one transcription per line.\n",
    "    '''\n",
    "    # load text file\n",
    "    lines = open(text_file, encoding=\"utf8\").readlines()\n",
    "    # cleanup the text\n",
    "    lines = [clean_text(line) for line in lines]\n",
    "    # replace spaces with special token `|`\n",
    "    lines = ['|'.join(line.split(' ')) for line in lines]\n",
    "    # make all characters upper-case for wav2vec token outputs\n",
    "    lines = [line.upper() for line in lines]\n",
    "    return lines\n",
    "    \n",
    "    \n",
    "def load_model(model_name):\n",
    "    '''Loads a pytorch `model_name` from torchaudio.\n",
    "    '''\n",
    "    bundle = getattr(torchaudio.pipelines, model_name, None)\n",
    "    if bundle:\n",
    "        model = bundle.get_model()\n",
    "    else:\n",
    "        raise ValueError(f'Could not find model: {model_name}')\n",
    "    return model, bundle\n",
    "\n",
    "\n",
    "\n",
    "def get_emissions(model, audio):\n",
    "    '''Gets token probabilities from `model` for the speech given in `audio`.\n",
    "    '''\n",
    "    emissions, _ = model(audio)\n",
    "    emissions = torch.log_softmax(emissions, dim=-1)\n",
    "    return emissions\n",
    "\n",
    "\n",
    "def get_trellis(emission, tokens, blank_id=0):\n",
    "    num_frame = emission.size(0)\n",
    "    num_tokens = len(tokens)\n",
    "\n",
    "    # Trellis has extra diemsions for both time axis and tokens.\n",
    "    # The extra dim for tokens represents <SoS> (start-of-sentence)\n",
    "    # The extra dim for time axis is for simplification of the code.\n",
    "    trellis = torch.empty((num_frame + 1, num_tokens + 1))\n",
    "    trellis[0, 0] = 0\n",
    "    trellis[1:, 0] = torch.cumsum(emission[:, 0], 0)\n",
    "    trellis[0, -num_tokens:] = -float(\"inf\")\n",
    "    trellis[-num_tokens:, 0] = float(\"inf\")\n",
    "\n",
    "    for t in range(num_frame):\n",
    "        trellis[t + 1, 1:] = torch.maximum(\n",
    "            # Score for staying at the same token\n",
    "            trellis[t, 1:] + emission[t, blank_id],\n",
    "            # Score for changing to the next token\n",
    "            trellis[t, :-1] + emission[t, tokens],\n",
    "        )\n",
    "    return trellis\n",
    "\n",
    "\n",
    "def backtrack(trellis, emission, tokens, blank_id=0):\n",
    "    # Note:\n",
    "    # j and t are indices for trellis, which has extra dimensions\n",
    "    # for time and tokens at the beginning.\n",
    "    # When referring to time frame index `T` in trellis,\n",
    "    # the corresponding index in emission is `T-1`.\n",
    "    # Similarly, when referring to token index `J` in trellis,\n",
    "    # the corresponding index in transcript is `J-1`.\n",
    "    j = trellis.size(1) - 1\n",
    "    t_start = torch.argmax(trellis[:, j]).item()\n",
    "\n",
    "    path = []\n",
    "    for t in range(t_start, 0, -1):\n",
    "        # 1. Figure out if the current position was stay or change\n",
    "        # Note (again):\n",
    "        # `emission[J-1]` is the emission at time frame `J` of trellis dimension.\n",
    "        # Score for token staying the same from time frame J-1 to T.\n",
    "        stayed = trellis[t - 1, j] + emission[t - 1, blank_id]\n",
    "        # Score for token changing from C-1 at T-1 to J at T.\n",
    "        changed = trellis[t - 1, j - 1] + emission[t - 1, tokens[j - 1]]\n",
    "\n",
    "        # 2. Store the path with frame-wise probability.\n",
    "        prob = emission[t - 1, tokens[j - 1] if changed > stayed else 0].exp().item()\n",
    "        # Return token index and time index in non-trellis coordinate.\n",
    "        path.append(Point(j - 1, t - 1, prob))\n",
    "\n",
    "        # 3. Update the token\n",
    "        if changed > stayed:\n",
    "            j -= 1\n",
    "            if j == 0:\n",
    "                break\n",
    "    else:\n",
    "        raise ValueError(\"Failed to align\")\n",
    "    return path[::-1]\n",
    "\n",
    "\n",
    "def merge_repeats(path, transcript):\n",
    "    i1, i2 = 0, 0\n",
    "    segments = []\n",
    "    while i1 < len(path):\n",
    "        while i2 < len(path) and path[i1].token_index == path[i2].token_index:\n",
    "            i2 += 1\n",
    "        score = sum(path[k].score for k in range(i1, i2)) / (i2 - i1)\n",
    "        segments.append(\n",
    "            Segment(\n",
    "                transcript[path[i1].token_index],\n",
    "                path[i1].time_index,\n",
    "                path[i2 - 1].time_index + 1,\n",
    "                score,\n",
    "            )\n",
    "        )\n",
    "        i1 = i2\n",
    "    return segments\n",
    "\n",
    "\n",
    "# Merge words\n",
    "def merge_words(segments, separator=\"|\"):\n",
    "    words = []\n",
    "    i1, i2 = 0, 0\n",
    "    while i1 < len(segments):\n",
    "        if i2 >= len(segments) or segments[i2].label == separator:\n",
    "            if i1 != i2:\n",
    "                segs = segments[i1:i2]\n",
    "                word = \"\".join([seg.label for seg in segs])\n",
    "                score = sum(seg.score * seg.length for seg in segs) / sum(seg.length for seg in segs)\n",
    "                words.append(Segment(word, segments[i1].start, segments[i2 - 1].end, score))\n",
    "            i1 = i2 + 1\n",
    "            i2 = i1\n",
    "        else:\n",
    "            i2 += 1\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example run through\n",
    "\n",
    "\n",
    "def run_forced_alignment(\n",
    "    model_name,\n",
    "    speech_file,\n",
    "    text_file,\n",
    "    device=None,\n",
    "    \n",
    "):\n",
    "    \n",
    "    # set the hardware device\n",
    "    device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # load the model\n",
    "    model, bundle = load_model(model_name)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # get model labels\n",
    "    labels = bundle.get_labels()\n",
    "    dictionary = {c: i for i, c in enumerate(labels)}\n",
    "\n",
    "    # load the audio and transcript\n",
    "    audio = load_audio(speech_file)\n",
    "    audio = audio.to(device)\n",
    "    \n",
    "    # read in the transcript\n",
    "    if os.path.isfile(text_file):\n",
    "        transcript = load_transcript(text_file)[0]\n",
    "    else:\n",
    "        print(f'Note: using \"{text_file}\" as the transcript')\n",
    "        transcript = text_file\n",
    "\n",
    "    # get the token probabilities\n",
    "    emissions = get_emissions(model, audio)\n",
    "    emissions = emissions[0].detach().cpu()\n",
    "\n",
    "    # turn the transcript into tokens\n",
    "    tokens = [dictionary[c] for c in transcript]\n",
    "\n",
    "    # populate the trellis\n",
    "    trellis = get_trellis(emissions, tokens)\n",
    "\n",
    "    # walkback to find the most likely trellis path\n",
    "    path = backtrack(trellis, emissions, tokens)\n",
    "\n",
    "    # merge the paths with repeated labels\n",
    "    segments = merge_repeats(path, transcript)\n",
    "\n",
    "    # merge the words based on the separator token '|'\n",
    "    word_segments = merge_words(segments)\n",
    "    \n",
    "    return {\n",
    "        'character_segs': segments,\n",
    "        'word_segs': word_segments,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "segs = run_forced_alignment(\n",
    "    MODEL_NAME,\n",
    "    SPEECH_FILE,\n",
    "    TEXT_FILE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[F\t(1.00): [   20,    27),\n",
       " O\t(1.00): [   27,    28),\n",
       " U\t(0.99): [   28,    30),\n",
       " R\t(0.63): [   30,    34),\n",
       " |\t(0.94): [   34,    37),\n",
       " S\t(1.00): [   37,    41),\n",
       " C\t(1.00): [   41,    49),\n",
       " O\t(1.00): [   49,    51),\n",
       " R\t(1.00): [   51,    53),\n",
       " E\t(1.00): [   53,    55),\n",
       " |\t(1.00): [   55,    58),\n",
       " A\t(1.00): [   58,    59),\n",
       " N\t(1.00): [   59,    60),\n",
       " D\t(0.52): [   60,    62),\n",
       " |\t(1.00): [   62,    64),\n",
       " S\t(1.00): [   64,    68),\n",
       " E\t(1.00): [   68,    71),\n",
       " V\t(1.00): [   71,    73),\n",
       " E\t(0.93): [   73,    75),\n",
       " N\t(0.86): [   75,    77),\n",
       " |\t(1.00): [   77,    78),\n",
       " Y\t(1.00): [   78,    79),\n",
       " E\t(0.50): [   79,    81),\n",
       " A\t(0.83): [   81,    83),\n",
       " R\t(1.00): [   83,    86),\n",
       " S\t(1.00): [   86,    88),\n",
       " |\t(1.00): [   88,    90),\n",
       " A\t(1.00): [   90,    93),\n",
       " G\t(1.00): [   93,    97),\n",
       " O\t(1.00): [   97,   100),\n",
       " |\t(0.96): [  100,   124),\n",
       " O\t(1.00): [  124,   125),\n",
       " U\t(1.00): [  125,   127),\n",
       " R\t(0.88): [  127,   129),\n",
       " |\t(1.00): [  129,   132),\n",
       " F\t(1.00): [  132,   138),\n",
       " A\t(1.00): [  138,   142),\n",
       " T\t(1.00): [  142,   143),\n",
       " H\t(1.00): [  143,   145),\n",
       " E\t(1.00): [  145,   147),\n",
       " R\t(0.99): [  147,   150),\n",
       " S\t(0.52): [  150,   152),\n",
       " |\t(1.00): [  152,   156),\n",
       " B\t(1.00): [  156,   158),\n",
       " R\t(1.00): [  158,   160),\n",
       " O\t(1.00): [  160,   161),\n",
       " U\t(0.84): [  161,   163),\n",
       " G\t(1.00): [  163,   164),\n",
       " H\t(1.00): [  164,   165),\n",
       " T\t(0.52): [  165,   168),\n",
       " |\t(1.00): [  168,   170),\n",
       " F\t(0.83): [  170,   175),\n",
       " O\t(0.53): [  175,   177),\n",
       " R\t(1.00): [  177,   180),\n",
       " T\t(1.00): [  180,   181),\n",
       " H\t(0.98): [  181,   183),\n",
       " |\t(0.71): [  183,   186),\n",
       " O\t(0.99): [  186,   187),\n",
       " N\t(0.97): [  187,   189),\n",
       " |\t(1.00): [  189,   191),\n",
       " T\t(1.00): [  191,   192),\n",
       " H\t(1.00): [  192,   193),\n",
       " I\t(0.69): [  193,   196),\n",
       " S\t(0.57): [  196,   198),\n",
       " |\t(1.00): [  198,   200),\n",
       " C\t(1.00): [  200,   205),\n",
       " O\t(1.00): [  205,   207),\n",
       " N\t(1.00): [  207,   209),\n",
       " T\t(0.77): [  209,   213),\n",
       " I\t(1.00): [  213,   215),\n",
       " N\t(1.00): [  215,   218),\n",
       " E\t(1.00): [  218,   219),\n",
       " N\t(1.00): [  219,   221),\n",
       " T\t(0.80): [  221,   222),\n",
       " |\t(0.00): [  222,   223),\n",
       " |\t(1.00): [  223,   232),\n",
       " A\t(0.54): [  232,   234),\n",
       " |\t(1.00): [  234,   237),\n",
       " N\t(1.00): [  237,   239),\n",
       " E\t(1.00): [  239,   240),\n",
       " W\t(0.67): [  240,   243),\n",
       " |\t(1.00): [  243,   245),\n",
       " N\t(1.00): [  245,   249),\n",
       " A\t(1.00): [  249,   253),\n",
       " T\t(1.00): [  253,   255),\n",
       " I\t(1.00): [  255,   257),\n",
       " O\t(1.00): [  257,   258),\n",
       " N\t(1.00): [  258,   260),\n",
       " |\t(1.00): [  260,   261),\n",
       " |\t(0.95): [  261,   283),\n",
       " C\t(1.00): [  283,   286),\n",
       " O\t(1.00): [  286,   287),\n",
       " N\t(1.00): [  287,   291),\n",
       " C\t(1.00): [  291,   296),\n",
       " E\t(1.00): [  296,   297),\n",
       " I\t(0.50): [  297,   299),\n",
       " V\t(1.00): [  299,   301),\n",
       " E\t(1.00): [  301,   302),\n",
       " D\t(0.64): [  302,   304),\n",
       " |\t(0.83): [  304,   306),\n",
       " I\t(0.99): [  306,   307),\n",
       " N\t(0.60): [  307,   309),\n",
       " |\t(1.00): [  309,   310),\n",
       " L\t(1.00): [  310,   312),\n",
       " I\t(1.00): [  312,   315),\n",
       " B\t(1.00): [  315,   316),\n",
       " E\t(1.00): [  316,   318),\n",
       " R\t(1.00): [  318,   320),\n",
       " T\t(0.96): [  320,   324),\n",
       " Y\t(1.00): [  324,   325),\n",
       " |\t(1.00): [  325,   326),\n",
       " |\t(0.90): [  326,   336),\n",
       " A\t(1.00): [  336,   337),\n",
       " N\t(1.00): [  337,   339),\n",
       " D\t(1.00): [  339,   340),\n",
       " |\t(0.61): [  340,   342),\n",
       " D\t(1.00): [  342,   345),\n",
       " E\t(1.00): [  345,   349),\n",
       " D\t(1.00): [  349,   351),\n",
       " I\t(1.00): [  351,   354),\n",
       " C\t(1.00): [  354,   357),\n",
       " A\t(1.00): [  357,   360),\n",
       " T\t(1.00): [  360,   362),\n",
       " E\t(1.00): [  362,   363),\n",
       " D\t(1.00): [  363,   365),\n",
       " |\t(0.54): [  365,   367),\n",
       " T\t(1.00): [  367,   368),\n",
       " O\t(1.00): [  368,   370),\n",
       " |\t(1.00): [  370,   371),\n",
       " T\t(1.00): [  371,   372),\n",
       " H\t(1.00): [  372,   373),\n",
       " E\t(0.71): [  373,   375),\n",
       " |\t(1.00): [  375,   377),\n",
       " P\t(1.00): [  377,   378),\n",
       " R\t(0.67): [  378,   381),\n",
       " O\t(1.00): [  381,   385),\n",
       " P\t(1.00): [  385,   388),\n",
       " O\t(0.92): [  388,   391),\n",
       " S\t(1.00): [  391,   394),\n",
       " I\t(1.00): [  394,   397),\n",
       " T\t(1.00): [  397,   399),\n",
       " I\t(1.00): [  399,   401),\n",
       " O\t(1.00): [  401,   402),\n",
       " N\t(0.67): [  402,   405),\n",
       " |\t(1.00): [  405,   408),\n",
       " T\t(1.00): [  408,   409),\n",
       " H\t(1.00): [  409,   411),\n",
       " A\t(1.00): [  411,   412),\n",
       " T\t(0.88): [  412,   414),\n",
       " |\t(1.00): [  414,   420),\n",
       " A\t(1.00): [  420,   421),\n",
       " L\t(1.00): [  421,   425),\n",
       " L\t(1.00): [  425,   426),\n",
       " |\t(0.74): [  426,   429),\n",
       " M\t(1.00): [  429,   435),\n",
       " E\t(1.00): [  435,   436),\n",
       " N\t(0.67): [  436,   439),\n",
       " |\t(1.00): [  439,   447),\n",
       " A\t(1.00): [  447,   449),\n",
       " R\t(1.00): [  449,   450),\n",
       " E\t(0.51): [  450,   452),\n",
       " |\t(1.00): [  452,   453),\n",
       " C\t(0.77): [  453,   457),\n",
       " R\t(1.00): [  457,   459),\n",
       " E\t(1.00): [  459,   463),\n",
       " A\t(1.00): [  463,   466),\n",
       " T\t(1.00): [  466,   469),\n",
       " E\t(1.00): [  469,   470),\n",
       " D\t(1.00): [  470,   472),\n",
       " |\t(0.89): [  472,   481),\n",
       " E\t(1.00): [  481,   484),\n",
       " Q\t(1.00): [  484,   487),\n",
       " U\t(1.00): [  487,   490),\n",
       " A\t(1.00): [  490,   491),\n",
       " L\t(0.98): [  491,   493),\n",
       " |\t(0.98): [  493,   494)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segs['character_segs']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
