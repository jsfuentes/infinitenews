{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f638640f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63b4c35",
   "metadata": {},
   "source": [
    "## Creating Audio File\n",
    "\n",
    "Checkout https://beta.elevenlabs.io/history for full log of audio creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e508c1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"./kitten_full\"\n",
    "mp3_file_name = file_name + \".mp3\"\n",
    "wav_file_name = file_name + \".wav\"\n",
    "\n",
    "generate_new_audio = False\n",
    "name_of_voice = \"Bella\"\n",
    "text = \"I love you\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1e5c7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'voice_id': 'EXAVITQu4vr4xnSDxMaL', 'name': 'Bella', 'samples': None, 'category': 'premade', 'fine_tuning': {'is_allowed_to_fine_tune': False, 'fine_tuning_requested': False, 'finetuning_state': 'not_started', 'verification_attempts': None, 'verification_attempts_count': 0}, 'labels': {}, 'preview_url': 'https://storage.googleapis.com/eleven-public-prod/premade/voices/EXAVITQu4vr4xnSDxMaL/04365bce-98cc-4e99-9f10-56b60680cda9.mp3', 'available_for_tiers': [], 'settings': None}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'EXAVITQu4vr4xnSDxMaL'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get voice ID\n",
    "import requests\n",
    "\n",
    "r1 = requests.get('https://api.elevenlabs.io/v1/voices')\n",
    "resp = r1.json()\n",
    "rid = \"\"\n",
    "for voice in resp[\"voices\"]:\n",
    "    if voice[\"name\"] == name_of_voice:\n",
    "        print(voice)\n",
    "        rid = voice[\"voice_id\"]\n",
    "        break;\n",
    "rid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5589f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get audio file, minimize runs cost money\n",
    "import json\n",
    "\n",
    "if generate_new_audio:\n",
    "    url = \"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}\".format(voice_id=rid)\n",
    "    print(url)\n",
    "    headers = {'xi-api-key': \"aa42307ce6bfbefcfd2abd4d8634127b\"}\n",
    "    payload = {\n",
    "      \"text\": text,\n",
    "      \"voice_settings\": {\n",
    "        \"stability\": 0.3,\n",
    "        \"similarity_boost\": 0.75\n",
    "      }\n",
    "    }\n",
    "\n",
    "    r2 = requests.post(url, data=json.dumps(payload), headers=headers)\n",
    "    print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c7a7580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and play audio file\n",
    "import os\n",
    "\n",
    "if generate_new_audio:\n",
    "    with open(mp3_file_name, 'wb') as f:\n",
    "        f.write(r2.content)\n",
    "\n",
    "    os.system(\"mpg123 \" + mp3_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a63bf150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert mp3 to wav                                                            \n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "\n",
    "if generate_new_audio:\n",
    "    sound = AudioSegment.from_mp3(mp3_file_name)\n",
    "    sound.export(wav_file_name, format=\"wav\")\n",
    "\n",
    "    # test sound\n",
    "    song = AudioSegment.from_wav(wav_file_name)\n",
    "    play(song)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbbc91a",
   "metadata": {},
   "source": [
    "## Generate phonemes\n",
    "- Uses https://github.com/m-bain/whisperX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f0226e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/enzokro/projects/fractal_hack/repos/infinitenews/ForcedAlignment\n",
      "File name: ./kitten_full.wav\n",
      "Base name: kitten_full.wav\n"
     ]
    }
   ],
   "source": [
    "# no extension, must be wav\n",
    "import os\n",
    "cwd = os.getcwd() #pwd\n",
    "print(cwd)\n",
    "\n",
    "audio_file_name = wav_file_name\n",
    "base_audio_file_name = os.path.basename(audio_file_name)\n",
    "print(\"File name:\", audio_file_name)\n",
    "print(\"Base name:\", base_audio_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b8e7fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import whisperx\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# transcribe with original whisper\n",
    "model = whisperx.load_model(\"large\", device)\n",
    "# model = whisperx.load_model(\"base.en\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c63bb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/enzokro/infinitenews/lib/python3.9/site-packages/whisperx/transcribe.py:83: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    }
   ],
   "source": [
    "result = model.transcribe(audio_file_name)\n",
    "print(result[\"segments\"]) # before alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcdf496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load alignment model and metadata\n",
    "model_a, metadata = whisperx.load_align_model(language_code=result[\"language\"], device=device)\n",
    "\n",
    "# align whisper output\n",
    "result_aligned = whisperx.align(result[\"segments\"], model_a, metadata, audio_file_name, device)\n",
    "\n",
    "# print(result_aligned[\"segments\"]) # after alignment\n",
    "result_aligned[\"word_segments\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a501c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_words = [\"that's\", \"Help! \", \"multiverse\", \"cat\", \"permit\", \"surprise\", \",whales\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c840961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "try:\n",
    "    arpabet = nltk.corpus.cmudict.dict()\n",
    "except LookupError:\n",
    "    nltk.download('cmudict')\n",
    "    arpabet = nltk.corpus.cmudict.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1074b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from functools import lru_cache\n",
    "from itertools import product as iterprod\n",
    "\n",
    "def cleanWord(word):\n",
    "    regex = r\"[^\\w\\']\"\n",
    "    return re.sub(regex, \"\", word).lower()\n",
    "\n",
    "def arpabetWithoutNumbers(word):\n",
    "    list_of_phones = []\n",
    "    for phones in arpabet[word]:\n",
    "        new_phones = [\"\".join(filter(lambda x: x.isalpha(), phone)) for phone in phones]\n",
    "        list_of_phones.append(new_phones)\n",
    "    return list_of_phones\n",
    "\n",
    "def wordbreak(s):\n",
    "    s = cleanWord(s)\n",
    "    if s in arpabet:\n",
    "        return arpabetWithoutNumbers(s)\n",
    "    middle = len(s)/2\n",
    "    partition = sorted(list(range(len(s))), key=lambda x: (x-middle)**2-x)\n",
    "    for i in partition:\n",
    "        pre, suf = (s[:i], s[i:])\n",
    "        if pre in arpabet and wordbreak(suf) is not None:\n",
    "            return [x+y for x,y in iterprod(arpabetWithoutNumbers(pre), wordbreak(suf))]\n",
    "    return None\n",
    "\n",
    "# Example words: \n",
    "for w in test_words:\n",
    "    print(wordbreak(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d31522e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def letterToPhone(word, phones):\n",
    "    # print(\"LTP\", word, phones)\n",
    "    if word == \"\":\n",
    "        return []\n",
    "\n",
    "    for i in reversed(range(1, len(word))):\n",
    "        for j in range(0, len(word) - i + 1):\n",
    "            subword = word[j:i+j]\n",
    "            cur_possible_phones = wordbreak(subword)\n",
    "            #For when has contractions\n",
    "            if not cur_possible_phones:\n",
    "#                 print(\"Skipping\", i, j, subword, cur_possible_phones)\n",
    "                continue\n",
    "            else:\n",
    "                cur_possible_phones = cur_possible_phones + [[subword.upper()]]\n",
    "#                 print(i, j, subword, cur_possible_phones)\n",
    "\n",
    "            for cur_phones in cur_possible_phones:\n",
    "                extra_length = len(phones) - len(cur_phones)\n",
    "#                 print(\"EL\", extra_length)\n",
    "                if extra_length > 0:\n",
    "                    for a in range(0, extra_length + 1):\n",
    "                        subphones_end = len(phones) - (extra_length - a)\n",
    "                        subphones = phones[a:subphones_end]\n",
    "                        if len(subphones) != len(cur_phones):\n",
    "                            print(\"Lengths not equal....\")\n",
    "                        #print(subphones, cur_phones)\n",
    "                        if subphones == cur_phones:\n",
    "                            #print(\"MATCH\", subword, subphones)\n",
    "                            first = letterToPhone(word[0:j], phones[0:a])\n",
    "                            cur = letterToPhone(subword, subphones)\n",
    "                            last = letterToPhone(word[i+j:], phones[subphones_end:])\n",
    "                            # print(\"LTP Complete\", first, cur, last)\n",
    "                            return first + cur + last\n",
    "                    \n",
    "    return [[word.upper(), phones]]\n",
    "\n",
    "for segment in result_aligned[\"word_segments\"]:\n",
    "    word = segment[\"text\"]\n",
    "# for word in test_words:\n",
    "#     print(word)\n",
    "#     print(letterToPhone(word, wordbreak(word)[0]))\n",
    "#     print([word, wordbreak(word)[0]], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9907b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_words = []\n",
    "for segment in result_aligned[\"word_segments\"]:\n",
    "    word = segment[\"text\"]\n",
    "    chunked_words.append(letterToPhone(word, wordbreak(word)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985f8669",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_words[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f5d66b",
   "metadata": {},
   "source": [
    "# Parsing out the transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249d18ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = [o['text'] for o in result_aligned[\"word_segments\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b181205",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = '|'.join(ts).upper()\n",
    "ts = ts.replace(',', '')\n",
    "ts = ts.replace('.', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ad04db",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef202f5c",
   "metadata": {},
   "source": [
    "# FORCED ALIGNMENT FROM WAV2VEC2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2914fbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams[\"figure.figsize\"] = [16.0, 4.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418ae637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model we are loading\n",
    "MODEL_NAME = 'WAV2VEC2_ASR_LARGE_LV60K_960H'\n",
    "# MODEL_NAME = 'WAV2VEC2_ASR_BASE_960H'\n",
    "# MODEL_NAME = 'WAV2VEC2_ASR_LARGE_960H'\n",
    "# sampling rate the model expects\n",
    "# NOTE: most wav2vec models assume 16k sampling rate\n",
    "MODEL_SR = 16_000\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Point:\n",
    "    token_index: int\n",
    "    time_index: int\n",
    "    score: float\n",
    "\n",
    "\n",
    "# Merge the labels\n",
    "@dataclass\n",
    "class Segment:\n",
    "    label: str\n",
    "    start: int\n",
    "    end: int\n",
    "    score: float\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.label}\\t({self.score:4.2f}): [{self.start:5f}, {self.end:5f})\"\n",
    "\n",
    "    @property\n",
    "    def length(self):\n",
    "        return self.end - self.start\n",
    "\n",
    "\n",
    "\n",
    "def load_audio(wav_file, model_sr=MODEL_SR):\n",
    "    '''Loads `wav_file` using torchaudio.\n",
    "    \n",
    "    Resamples the audio to `MODEL_SR` if needed.\n",
    "    '''\n",
    "    wav, sr = torchaudio.load(wav_file)\n",
    "    if sr != model_sr:\n",
    "        print(f'Resampling from {sr} to {MODEL_SR}')\n",
    "        wav = torchaudio.transforms.Resample(sr, MODEL_SR)(wav)\n",
    "    return wav\n",
    "\n",
    "\n",
    "def clean_text(o):\n",
    "    clean = o.replace(',', ' ')\n",
    "    clean = clean.strip('\\n')\n",
    "    return clean\n",
    "\n",
    "def load_transcript(text_file):\n",
    "    '''Loads a transcript in `text_file`.\n",
    "    \n",
    "    Assumes one transcription per line.\n",
    "    '''\n",
    "    # load text file\n",
    "    lines = open(text_file, encoding=\"utf8\").readlines()\n",
    "    # cleanup the text\n",
    "    lines = [clean_text(line) for line in lines]\n",
    "    # replace spaces with special token `|`\n",
    "    lines = ['|'.join(line.split(' ')) for line in lines]\n",
    "    # make all characters upper-case for wav2vec token outputs\n",
    "    lines = [line.upper() for line in lines]\n",
    "    return lines\n",
    "    \n",
    "    \n",
    "def load_model(model_name):\n",
    "    '''Loads a pytorch `model_name` from torchaudio.\n",
    "    '''\n",
    "    bundle = getattr(torchaudio.pipelines, model_name, None)\n",
    "    if bundle:\n",
    "        model = bundle.get_model()\n",
    "    else:\n",
    "        raise ValueError(f'Could not find model: {model_name}')\n",
    "    return model, bundle\n",
    "\n",
    "\n",
    "\n",
    "def get_emissions(model, audio):\n",
    "    '''Gets token probabilities from `model` for the speech given in `audio`.\n",
    "    '''\n",
    "    emissions, _ = model(audio)\n",
    "    emissions = torch.log_softmax(emissions, dim=-1)\n",
    "    return emissions\n",
    "\n",
    "\n",
    "def get_trellis(emission, tokens, blank_id=0):\n",
    "    num_frame = emission.size(0)\n",
    "    num_tokens = len(tokens)\n",
    "\n",
    "    # Trellis has extra diemsions for both time axis and tokens.\n",
    "    # The extra dim for tokens represents <SoS> (start-of-sentence)\n",
    "    # The extra dim for time axis is for simplification of the code.\n",
    "    trellis = torch.empty((num_frame + 1, num_tokens + 1))\n",
    "    trellis[0, 0] = 0\n",
    "    trellis[1:, 0] = torch.cumsum(emission[:, 0], 0)\n",
    "    trellis[0, -num_tokens:] = -float(\"inf\")\n",
    "    trellis[-num_tokens:, 0] = float(\"inf\")\n",
    "\n",
    "    for t in range(num_frame):\n",
    "        trellis[t + 1, 1:] = torch.maximum(\n",
    "            # Score for staying at the same token\n",
    "            trellis[t, 1:] + emission[t, blank_id],\n",
    "            # Score for changing to the next token\n",
    "            trellis[t, :-1] + emission[t, tokens],\n",
    "        )\n",
    "    return trellis\n",
    "\n",
    "\n",
    "def backtrack(trellis, emission, tokens, blank_id=0):\n",
    "    # Note:\n",
    "    # j and t are indices for trellis, which has extra dimensions\n",
    "    # for time and tokens at the beginning.\n",
    "    # When referring to time frame index `T` in trellis,\n",
    "    # the corresponding index in emission is `T-1`.\n",
    "    # Similarly, when referring to token index `J` in trellis,\n",
    "    # the corresponding index in transcript is `J-1`.\n",
    "    j = trellis.size(1) - 1\n",
    "    t_start = torch.argmax(trellis[:, j]).item()\n",
    "\n",
    "    path = []\n",
    "    for t in range(t_start, 0, -1):\n",
    "        # 1. Figure out if the current position was stay or change\n",
    "        # Note (again):\n",
    "        # `emission[J-1]` is the emission at time frame `J` of trellis dimension.\n",
    "        # Score for token staying the same from time frame J-1 to T.\n",
    "        stayed = trellis[t - 1, j] + emission[t - 1, blank_id]\n",
    "        # Score for token changing from C-1 at T-1 to J at T.\n",
    "        changed = trellis[t - 1, j - 1] + emission[t - 1, tokens[j - 1]]\n",
    "\n",
    "        # 2. Store the path with frame-wise probability.\n",
    "        prob = emission[t - 1, tokens[j - 1] if changed > stayed else 0].exp().item()\n",
    "        # Return token index and time index in non-trellis coordinate.\n",
    "        path.append(Point(j - 1, t - 1, prob))\n",
    "\n",
    "        # 3. Update the token\n",
    "        if changed > stayed:\n",
    "            j -= 1\n",
    "            if j == 0:\n",
    "                break\n",
    "    else:\n",
    "        raise ValueError(\"Failed to align\")\n",
    "    return path[::-1]\n",
    "\n",
    "\n",
    "def merge_repeats(path, transcript):\n",
    "    i1, i2 = 0, 0\n",
    "    segments = []\n",
    "    while i1 < len(path):\n",
    "        while i2 < len(path) and path[i1].token_index == path[i2].token_index:\n",
    "            i2 += 1\n",
    "        score = sum(path[k].score for k in range(i1, i2)) / (i2 - i1)\n",
    "        segments.append(\n",
    "            Segment(\n",
    "                transcript[path[i1].token_index],\n",
    "                path[i1].time_index,\n",
    "                path[i2 - 1].time_index + 1,\n",
    "                score,\n",
    "            )\n",
    "        )\n",
    "        i1 = i2\n",
    "    return segments\n",
    "\n",
    "\n",
    "# Merge words\n",
    "def merge_words(segments, separator=\"|\"):\n",
    "    words = []\n",
    "    i1, i2 = 0, 0\n",
    "    while i1 < len(segments):\n",
    "        if i2 >= len(segments) or segments[i2].label == separator:\n",
    "            if i1 != i2:\n",
    "                segs = segments[i1:i2]\n",
    "                word = \"\".join([seg.label for seg in segs])\n",
    "                score = sum(seg.score * seg.length for seg in segs) / sum(seg.length for seg in segs)\n",
    "                words.append(Segment(word, segments[i1].start, segments[i2 - 1].end, score))\n",
    "            i1 = i2 + 1\n",
    "            i2 = i1\n",
    "        else:\n",
    "            i2 += 1\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9391b407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example run through\n",
    "\n",
    "\n",
    "def run_forced_alignment(\n",
    "    model_name,\n",
    "    speech_file,\n",
    "    text_file,\n",
    "    device=None,\n",
    "    \n",
    "):\n",
    "    \n",
    "    # set the hardware device\n",
    "    device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # load the model\n",
    "    model, bundle = load_model(model_name)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # get model labels\n",
    "    labels = bundle.get_labels()\n",
    "    dictionary = {c: i for i, c in enumerate(labels)}\n",
    "    print(f'Aligning wav2vec2 tokens:')\n",
    "    print('\\n'.join([f'{k}: {v}' for k,v in dictionary.items()]))\n",
    "\n",
    "    # load the audio and transcript\n",
    "    audio = load_audio(speech_file)\n",
    "    audio = audio.to(device)\n",
    "    print(f'Audio shape: {audio.shape}')\n",
    "    \n",
    "    # read in the transcript\n",
    "    if os.path.isfile(text_file):\n",
    "        transcript = load_transcript(text_file)[0]\n",
    "    else:\n",
    "        print(f'Note: using \"{text_file}\" as the transcript')\n",
    "        transcript = text_file\n",
    "        \n",
    "    # turn the transcript into tokens\n",
    "    tokens = [dictionary[c] for c in transcript]\n",
    "\n",
    "    # get the token probabilities\n",
    "    emissions = get_emissions(model, audio)\n",
    "    emissions = emissions[0].detach().cpu()\n",
    "\n",
    "    # populate the trellis\n",
    "    trellis = get_trellis(emissions, tokens)\n",
    "\n",
    "    # walkback to find the most likely trellis path\n",
    "    path = backtrack(trellis, emissions, tokens)\n",
    "\n",
    "    # merge the paths with repeated labels\n",
    "    segments = merge_repeats(path, transcript)\n",
    "\n",
    "    # merge the words based on the separator token '|'\n",
    "    word_segments = merge_words(segments)\n",
    "    \n",
    "    # cleanup the model\n",
    "    del model\n",
    "    model = None\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return {\n",
    "        'character_segs': segments,\n",
    "        'word_segs': word_segments,\n",
    "        'trellis': trellis,\n",
    "        'emissions': emissions,\n",
    "        'tokens': tokens,\n",
    "        'path': path,\n",
    "        'audio': audio,\n",
    "        'bundle': bundle,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ad3b09",
   "metadata": {},
   "source": [
    "### Cleaning up some GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082258cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf514526",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "model = None\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect() \n",
    "\n",
    "del model_a\n",
    "model_a = None\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb77d7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a42d6e0",
   "metadata": {},
   "source": [
    "### Calling the Force Alignment on the `kitten_full.wav` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13409b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_file = 'kitten_full.wav'\n",
    "text_file = ts\n",
    "model_name = MODEL_NAME\n",
    "\n",
    "\n",
    "segs = run_forced_alignment(\n",
    "    model_name,\n",
    "    speech_file,\n",
    "    text_file,\n",
    "    device='cpu',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d571b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# character segmentation map\n",
    "segs['character_segs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0427c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view a few chunked words\n",
    "chunked_words[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79356309",
   "metadata": {},
   "source": [
    "# HERE: MAPPING CHARACTER DURATIONS TO PHONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a8903a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# make a copy of the wav2vec2 character segments, to be safe\n",
    "char_segs = segs['character_segs'][:]\n",
    "# this pointer will align character segments with words\n",
    "char_ptr = 0\n",
    "\n",
    "# stores the info for the needed transcriptions\n",
    "full_info = []\n",
    "\n",
    "\n",
    "def frame_to_ms(frame):\n",
    "    '''Converts a wav2vec2 frame index into miliseconds.\n",
    "    '''\n",
    "    return int(frame * 1000 / 50) # * 20 <-- miliseconds/frame\n",
    "\n",
    "\n",
    "# step through the chunked, phoneme words\n",
    "for cword in chunked_words:\n",
    "\n",
    "    # phone alignments for this word\n",
    "    alns = []\n",
    "    \n",
    "    # cleanup punctuation from the current word\n",
    "    word = ''.join(o[0] for o in cword).lower()\n",
    "    word = word.replace(',', '')\n",
    "    word = word.replace('.', '')\n",
    "    \n",
    "    \n",
    "    # step through the pairs of letters/phones for this word\n",
    "    for (letters, phones) in cword:\n",
    "        \n",
    "        # cleanup punctuation from the letters\n",
    "        if letters:\n",
    "            letters = letters.replace(',', '')\n",
    "            letters = letters.replace('.', '')\n",
    "        \n",
    "        # group the character segments for this letter/phone pair\n",
    "        cur_segs = []\n",
    "        while ''.join(o.label for o in cur_segs) != letters and char_ptr < len(char_segs):\n",
    "            cur_segs.append(char_segs[char_ptr])\n",
    "            char_ptr += 1\n",
    "             \n",
    "        # add precise phone timestamps\n",
    "        if phones:\n",
    "            \n",
    "            if cur_segs:\n",
    "                # start and stop time of this current segment\n",
    "                beg_ms = cur_beg = frame_to_ms(cur_segs[0].start)\n",
    "                end_ms = frame_to_ms(cur_segs[-1].end)\n",
    "\n",
    "                # find the duration of each phone\n",
    "                dur = end_ms - beg_ms\n",
    "                dur_per_phone = dur / len(phones)\n",
    "                \n",
    "                # if there is more than one phone, the duration is spread equally\n",
    "                # else, the entire segment duration is allocated to the single phone\n",
    "                for idx,phone in enumerate(phones):\n",
    "                    cur_end = cur_beg + dur_per_phone\n",
    "                    if cur_segs:\n",
    "                        alns.append({\n",
    "                            'ph': phone,\n",
    "                            'bg': cur_beg,\n",
    "                            'ed': cur_end,\n",
    "                        })\n",
    "                    cur_beg = cur_end\n",
    "                \n",
    "                \n",
    "        # if no phones are given for this letter, then extend the previous phone's durations   \n",
    "        else:\n",
    "            # make sure we have phones and segments for coverage\n",
    "            if alns and cur_segs:\n",
    "                # extend the interval\n",
    "                prev_seg = alns[-1]\n",
    "                prev_seg['ed'] = frame_to_ms(cur_segs[-1].end)\n",
    "                alns[-1] = prev_seg\n",
    "                \n",
    "        \n",
    "    # add the info we phone timestamp info for this word\n",
    "    full_info.append({\n",
    "        'word': word,\n",
    "        'phones': alns\n",
    "    })\n",
    "            \n",
    "        \n",
    "    # check if we still have segments to process\n",
    "    if char_ptr < len(char_segs):\n",
    "        \n",
    "        # if we're at the end of a word, add a silence\n",
    "        if char_segs[char_ptr].label == '|':\n",
    "            char_ptr += 1\n",
    "            full_info.append({\n",
    "                'word': 'SIL',\n",
    "                'phones': {\n",
    "                    'ph': 'sil',\n",
    "                    'bg': full_info[-1]['phones'][-1]['ed'],\n",
    "                    'ed': frame_to_ms(char_segs[char_ptr].start),\n",
    "                }\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f28df7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4641f32e",
   "metadata": {},
   "source": [
    "# Trying to plot the segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777917e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trellis, segments, word_segments, waveform, bundle = segs['trellis'], segs['character_segs'], segs['word_segs'], segs['audio'], segs['bundle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b127c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_alignments(trellis, segments, word_segments, waveform, bundle):\n",
    "    trellis_with_path = trellis.clone()\n",
    "    for i, seg in enumerate(segments):\n",
    "        if seg.label != \"|\":\n",
    "            trellis_with_path[seg.start + 1 : seg.end + 1, i + 1] = float(\"nan\")\n",
    "\n",
    "    fig, [ax1, ax2] = plt.subplots(2, 1, figsize=(16, 9.5))\n",
    "\n",
    "    ax1.imshow(trellis_with_path[1:, 1:].T, origin=\"lower\")\n",
    "    ax1.set_xticks([])\n",
    "    ax1.set_yticks([])\n",
    "\n",
    "    for word in word_segments:\n",
    "        ax1.axvline(word.start - 0.5)\n",
    "        ax1.axvline(word.end - 0.5)\n",
    "\n",
    "    for i, seg in enumerate(segments):\n",
    "        if seg.label != \"|\":\n",
    "            ax1.annotate(seg.label, (seg.start, i + 0.3))\n",
    "            ax1.annotate(f\"{seg.score:.2f}\", (seg.start, i + 4), fontsize=8)\n",
    "\n",
    "    # The original waveform\n",
    "    ratio = waveform.size(0) / (trellis.size(0) - 1)\n",
    "    ax2.plot(waveform)\n",
    "    for word in word_segments:\n",
    "        x0 = ratio * word.start\n",
    "        x1 = ratio * word.end\n",
    "        ax2.axvspan(x0, x1, alpha=0.1, color=\"red\")\n",
    "        ax2.annotate(f\"{word.score:.2f}\", (x0, 0.8))\n",
    "\n",
    "    for seg in segments:\n",
    "        if seg.label != \"|\":\n",
    "            ax2.annotate(seg.label, (seg.start * ratio, 0.9))\n",
    "    xticks = ax2.get_xticks()\n",
    "    plt.xticks(xticks, xticks / bundle.sample_rate)\n",
    "    ax2.set_xlabel(\"time [second]\")\n",
    "    ax2.set_yticks([])\n",
    "    ax2.set_ylim(-1.0, 1.0)\n",
    "    ax2.set_xlim(0, waveform.size(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491b8a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_alignments(\n",
    "    trellis,\n",
    "    segments,\n",
    "    word_segments,\n",
    "    waveform[0],\n",
    "    bundle,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dfd7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A trick to embed the resulting audio to the generated file.\n",
    "# `IPython.display.Audio` has to be the last call in a cell,\n",
    "# and there should be only one call par cell.\n",
    "def display_segment(bundle, trellis, word_segments, i):\n",
    "    ratio = waveform.size(1) / (trellis.size(0) - 1)\n",
    "    word = word_segments[i]\n",
    "    x0 = int(ratio * word.start)\n",
    "    x1 = int(ratio * word.end)\n",
    "    print(f\"{word.label} ({word.score:.2f}): {x0 / bundle.sample_rate:.3f} - {x1 / bundle.sample_rate:.3f} sec\")\n",
    "    segment = waveform[:, x0:x1]\n",
    "    return IPython.display.Audio(segment.numpy(), rate=bundle.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bec0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_helper(i):\n",
    "    return display_segment(bundle, trellis, word_segments, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d37d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_helper(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e62282",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_helper(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb18e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_helper(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21070cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_helper(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede9e1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_helper(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5275a061",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_helper(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50f32c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_helper(-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f940ddb",
   "metadata": {},
   "source": [
    "### Group into needed format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cce32b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af76b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "segs['character_segs'][-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d14d57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a9823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f89b76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(o.start / 50, o.end / 50) for o in segs['word_segs']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464ad372",
   "metadata": {},
   "source": [
    "# Comparing the alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a87f10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_aligned[\"word_segments\"];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec76a7be",
   "metadata": {},
   "source": [
    "### NODE: older code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ad6670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'text': 'Good', 'start': 0.10120481927710842, 'end': 0.32385542168674697},\n",
    "# INTO\n",
    "# [\n",
    "#     {\n",
    "#         \"word\": \"sil\",\n",
    "#         \"phones\": [\n",
    "#             {\n",
    "#                 \"ph\": \"SIL\",\n",
    "#                 \"bg\": 0,\n",
    "#                 \"ed\": 126\n",
    "#             }\n",
    "#         ]\n",
    "#     },\n",
    "#     {\n",
    "#         \"word\": \"YOU\",\n",
    "#         \"phones\": [\n",
    "#             {\n",
    "#                 \"ph\": \"Y\",\n",
    "#                 \"bg\": 126,\n",
    "#                 \"ed\": 138\n",
    "#             },\n",
    "#             {\n",
    "#                 \"ph\": \"UW\",\n",
    "#                 \"bg\": 138,\n",
    "#                 \"ed\": 141\n",
    "#             }\n",
    "#         ]\n",
    "#     },\n",
    "from math import trunc\n",
    "\n",
    "def makeSegment(word, phones, start, end):\n",
    "    total = end - start\n",
    "    time_per_phone = trunc(total / len(phones))\n",
    "#     print(total, time_per_phone)\n",
    "    expanded_phones = [{\"ph\": phone, \"bg\": start + (i * time_per_phone), \"ed\": start + ((i + 1) * time_per_phone)} for i, phone in enumerate(phones)]\n",
    "    return {\"word\": word, \"phones\": expanded_phones}\n",
    "\n",
    "def makeSil(start, end):\n",
    "    return {\"word\": \"sil\", \"phones\": [{\"ph\": \"SIL\", \"bg\": start, \"ed\": end}]}\n",
    "  \n",
    "all_phonemes = []\n",
    "for segment in result_aligned[\"word_segments\"]:\n",
    "    word = segment[\"text\"]\n",
    "    #Get rid of numbers in phones\n",
    "    phonemes = wordbreak(word)[0]\n",
    "    print(phonemes)\n",
    "    start_time = trunc(segment[\"start\"] * 100)\n",
    "    end_time = trunc(segment[\"end\"] * 100)\n",
    "    \n",
    "    if len(all_phonemes) > 0:\n",
    "        last = all_phonemes[-1]\n",
    "        last_end = last[\"phones\"][-1][\"ed\"]\n",
    "        if start_time - last_end > 5:\n",
    "            sil = makeSil(last_end, start_time)\n",
    "            all_phonemes.append(sil)\n",
    "        else:\n",
    "            start_time = last_end\n",
    "    else:\n",
    "        sil = makeSil(0, start_time)\n",
    "        all_phonemes.append(sil)\n",
    "        \n",
    "    segment = makeSegment(word, phonemes, start_time, end_time)\n",
    "    all_phonemes.append(segment)\n",
    "\n",
    "all_phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dd754f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# file_name = \"../samples/phonemes/{name}.json\".format(name=audio_file_name)\n",
    "# print(\"Writing to \", file_name)\n",
    "# f1 = open(file_name, 'w')\n",
    "# json.dump(all_phonemes, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4326767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb2d9d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de9ac94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f6aad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0c59c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64db7e3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce09a563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c7e085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43f5154",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90109a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
