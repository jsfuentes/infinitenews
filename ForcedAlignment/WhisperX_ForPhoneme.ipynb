{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f638640f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63b4c35",
   "metadata": {},
   "source": [
    "## Creating Audio File\n",
    "\n",
    "Checkout https://beta.elevenlabs.io/history for full log of audio creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "e508c1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eleven_labs_api = \"aa42307ce6bfbefcfd2abd4d8634127b\"\n",
    "file_name = \"./obama_intro\"\n",
    "mp3_file_name = f\"../AAAI22-one-shot-talking-face/samples/audios/{file_name}.mp3\"\n",
    "wav_file_name = f\"../AAAI22-one-shot-talking-face/samples/audios/{file_name}.wav\"\n",
    "\n",
    "generate_new_audio = False\n",
    "name_of_voice = \"T\"\n",
    "text = \"Good evening, everyone. Donald Trump here bringing you the latest news from the upside down world. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "e1e5c7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'voice_id': 'hAvYeODVB0NpGQX2vvYo', 'name': 'T', 'samples': [{'sample_id': 'MUq3YryANBS8VuL1vxv4', 'file_name': 'trump_strong.mp3', 'mime_type': 'audio/mpeg', 'size_bytes': 826788, 'hash': '031b0827047c91bd77243c3a9d7813c3'}, {'sample_id': 'Ug3mHySfs2ubWOjd69mq', 'file_name': 'Trump_WEF_2018 (mp3cut.net).mp3', 'mime_type': 'audio/mpeg', 'size_bytes': 4900013, 'hash': 'e5154651e4aa8d0aba74fd33cd5ea280'}, {'sample_id': 'jfBQ1LOIbChk30UoFwa1', 'file_name': 'trump.mp3', 'mime_type': 'audio/mpeg', 'size_bytes': 1856140, 'hash': 'ce297ae4d8ce548be4528a4a56afa98c'}], 'category': 'cloned', 'fine_tuning': {'is_allowed_to_fine_tune': False, 'fine_tuning_requested': False, 'finetuning_state': 'not_started', 'verification_attempts': None, 'verification_attempts_count': 0}, 'labels': {}, 'preview_url': 'https://storage.googleapis.com/eleven-public-prod/8t6uB5rLIfPz6K2jM5kLIdrKZSl1/voices/hAvYeODVB0NpGQX2vvYo/9cb2beb3-b2aa-4bc0-aa46-62f3b3559a96.mp3', 'available_for_tiers': [], 'settings': None}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hAvYeODVB0NpGQX2vvYo'"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get voice ID\n",
    "import requests\n",
    "\n",
    "headers = {'xi-api-key': eleven_labs_api}\n",
    "r1 = requests.get('https://api.elevenlabs.io/v1/voices', headers=headers)\n",
    "resp = r1.json()\n",
    "rid = \"\"\n",
    "for voice in resp[\"voices\"]:\n",
    "    if voice[\"name\"] == name_of_voice:\n",
    "        print(voice)\n",
    "        rid = voice[\"voice_id\"]\n",
    "        break;\n",
    "rid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "b5589f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.elevenlabs.io/v1/text-to-speech/hAvYeODVB0NpGQX2vvYo\n",
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "# Get audio file, minimize runs cost money\n",
    "import json\n",
    "\n",
    "if generate_new_audio:\n",
    "    url = \"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}\".format(voice_id=rid)\n",
    "    print(url)\n",
    "    headers = {'xi-api-key': eleven_labs_api}\n",
    "    payload = {\n",
    "      \"text\": text,\n",
    "      \"voice_settings\": {\n",
    "        \"stability\": 0,\n",
    "        \"similarity_boost\": 0.5\n",
    "      }\n",
    "    }\n",
    "\n",
    "    r2 = requests.post(url, data=json.dumps(payload), headers=headers)\n",
    "    print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "9c7a7580",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "High Performance MPEG 1.0/2.0/2.5 Audio Player for Layers 1, 2 and 3\n",
      "\tversion 1.31.2; written and copyright by Michael Hipp and others\n",
      "\tfree software (LGPL) without any warranty but with best wishes\n",
      "\n",
      "Directory: ../AAAI22-one-shot-talking-face/samples/audios/./\n",
      "Playing MPEG stream 1 of 1: trump_intro.mp3 ...\n",
      "\n",
      "MPEG 1.0 L III cbr64 44100 mono\n",
      "\n",
      "[0:05] Decoding of trump_intro.mp3 finished.\n"
     ]
    }
   ],
   "source": [
    "# Save and play audio file\n",
    "import os\n",
    "\n",
    "if generate_new_audio:\n",
    "    with open(mp3_file_name, 'wb') as f:\n",
    "        f.write(r2.content)\n",
    "\n",
    "    os.system(\"mpg123 \" + mp3_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "a63bf150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input #0, wav, from '/var/folders/g8/b65pczf97pb17p1_zrldsm500000gn/T/tmp0ov9bh8k.wav':\n",
      "  Duration: 00:00:05.93, bitrate: 705 kb/s\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 44100 Hz, 1 channels, s16, 705 kb/s\n",
      "   5.82 M-A:  0.000 fd=   0 aq=    0KB vq=    0KB sq=    0B f=0/0   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   5.86 M-A: -0.000 fd=   0 aq=    0KB vq=    0KB sq=    0B f=0/0   \r"
     ]
    }
   ],
   "source": [
    "# convert mp3 to wav                                                            \n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "\n",
    "if generate_new_audio:\n",
    "    sound = AudioSegment.from_mp3(mp3_file_name)\n",
    "    sound.export(wav_file_name, format=\"wav\")\n",
    "\n",
    "    # test sound\n",
    "    song = AudioSegment.from_wav(wav_file_name)\n",
    "    play(song)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbbc91a",
   "metadata": {},
   "source": [
    "## Generate phonemes\n",
    "- Uses https://github.com/m-bain/whisperX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "2f0226e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jfuentes/Projects/infinitenews/ForcedAlignment\n",
      "File name: ../AAAI22-one-shot-talking-face/samples/audios/./obama_intro.wav\n",
      "Base name: obama_intro\n"
     ]
    }
   ],
   "source": [
    "# no extension, must be wav\n",
    "import os\n",
    "cwd = os.getcwd() #pwd\n",
    "print(cwd)\n",
    "\n",
    "audio_file_name = wav_file_name\n",
    "base_audio_file_name = os.path.splitext(os.path.basename(audio_file_name))[0]\n",
    "print(\"File name:\", audio_file_name)\n",
    "print(\"Base name:\", base_audio_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "3b8e7fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import whisperx\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# transcribe with original whisper\n",
    "model = whisperx.load_model(\"medium.en\", device)\n",
    "# model = whisperx.load_model(\"base.en\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "adbc5ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jfuentes/Projects/infinitenews/AAAI22-one-shot-talking-face/venv/lib/python3.9/site-packages/whisperx/transcribe.py:83: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 0, 'seek': 0, 'start': 0.0, 'end': 7.0, 'text': ' Good evening, everyone. Obama here bringing you the latest news from the upside-down world.', 'tokens': [50363, 4599, 6180, 11, 2506, 13, 2486, 994, 6079, 345, 262, 3452, 1705, 422, 262, 17196, 12, 2902, 995, 13, 50713], 'temperature': 0.0, 'avg_logprob': -0.27768791805614124, 'compression_ratio': 1.0833333333333333, 'no_speech_prob': 0.03393666073679924}]\n"
     ]
    }
   ],
   "source": [
    "result = model.transcribe(audio_file_name)\n",
    "print(result[\"segments\"]) # before alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "5fcdf496",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jfuentes/Projects/infinitenews/AAAI22-one-shot-talking-face/venv/lib/python3.9/site-packages/whisperx/alignment.py:302: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  char_segments_arr = per_seg_grp.apply(lambda x: x.reset_index(drop = True)).reset_index()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'text': 'Good', 'start': 1.0429799426934099, 'end': 1.2034383954154728},\n",
       " {'text': 'evening,', 'start': 1.2836676217765044, 'end': 1.5644699140401148},\n",
       " {'text': 'everyone.', 'start': 1.6446991404011462, 'end': 2.025787965616046},\n",
       " {'text': 'Obama', 'start': 2.928366762177651, 'end': 3.3094555873925504},\n",
       " {'text': 'here', 'start': 3.349570200573066, 'end': 3.530085959885387},\n",
       " {'text': 'bringing', 'start': 3.5902578796561606, 'end': 3.891117478510029},\n",
       " {'text': 'you', 'start': 3.9312320916905446, 'end': 4.07163323782235},\n",
       " {'text': 'the', 'start': 4.111747851002866, 'end': 4.191977077363897},\n",
       " {'text': 'latest', 'start': 4.232091690544413, 'end': 4.553008595988539},\n",
       " {'text': 'news', 'start': 4.633237822349571, 'end': 4.914040114613181},\n",
       " {'text': 'from', 'start': 4.974212034383955, 'end': 5.134670487106018},\n",
       " {'text': 'the', 'start': 5.154727793696275, 'end': 5.255014326647565},\n",
       " {'text': 'upside-down', 'start': 5.335243553008596, 'end': 6.117478510028654},\n",
       " {'text': 'world.', 'start': 6.1977077363896855, 'end': 6.458452722063038}]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load alignment model and metadata\n",
    "model_a, metadata = whisperx.load_align_model(language_code=result[\"language\"], device=device)\n",
    "\n",
    "# align whisper output\n",
    "result_aligned = whisperx.align(result[\"segments\"], model_a, metadata, audio_file_name, device)\n",
    "\n",
    "# print(result_aligned[\"segments\"]) # after alignment\n",
    "result_aligned[\"word_segments\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "3a501c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_words = [\"that's\", \"Help! \", \"multiverse\", \"cat\", \"permit\", \"surprise\", \",whales\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "8c840961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "try:\n",
    "    arpabet = nltk.corpus.cmudict.dict()\n",
    "except LookupError:\n",
    "    nltk.download('cmudict')\n",
    "    arpabet = nltk.corpus.cmudict.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "d1074b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['DH', 'AE', 'T', 'S']]\n",
      "[['HH', 'EH', 'L', 'P']]\n",
      "[['M', 'AH', 'L', 'T', 'IY', 'V', 'ER', 'S']]\n",
      "[['K', 'AE', 'T']]\n",
      "[['P', 'ER', 'M', 'IH', 'T'], ['P', 'ER', 'M', 'IH', 'T']]\n",
      "[['S', 'ER', 'P', 'R', 'AY', 'Z'], ['S', 'AH', 'P', 'R', 'AY', 'Z']]\n",
      "[['W', 'EY', 'L', 'Z'], ['HH', 'W', 'EY', 'L', 'Z']]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from functools import lru_cache\n",
    "from itertools import product as iterprod\n",
    "\n",
    "def cleanWord(word):\n",
    "    regex = r\"[^\\w\\']\"\n",
    "    return re.sub(regex, \"\", word).lower()\n",
    "\n",
    "def arpabetWithoutNumbers(word):\n",
    "    list_of_phones = []\n",
    "    for phones in arpabet[word]:\n",
    "        new_phones = [\"\".join(filter(lambda x: x.isalpha(), phone)) for phone in phones]\n",
    "        list_of_phones.append(new_phones)\n",
    "    return list_of_phones\n",
    "\n",
    "def wordbreak(s):\n",
    "    s = cleanWord(s)\n",
    "    if s in arpabet:\n",
    "        return arpabetWithoutNumbers(s)\n",
    "    middle = len(s)/2\n",
    "    partition = sorted(list(range(len(s))), key=lambda x: (x-middle)**2-x)\n",
    "    for i in partition:\n",
    "        pre, suf = (s[:i], s[i:])\n",
    "        if pre in arpabet and wordbreak(suf) is not None:\n",
    "            return [x+y for x,y in iterprod(arpabetWithoutNumbers(pre), wordbreak(suf))]\n",
    "    return None\n",
    "\n",
    "# Example words: \n",
    "for w in test_words:\n",
    "    print(wordbreak(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "6d31522e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def letterToPhone(word, phones):\n",
    "    # print(\"LTP\", word, phones)\n",
    "    if word == \"\":\n",
    "        return []\n",
    "\n",
    "    for i in reversed(range(1, len(word))):\n",
    "        for j in range(0, len(word) - i + 1):\n",
    "            subword = word[j:i+j]\n",
    "            cur_possible_phones = wordbreak(subword)\n",
    "            #For when has contractions\n",
    "            if not cur_possible_phones:\n",
    "#                 print(\"Skipping\", i, j, subword, cur_possible_phones)\n",
    "                continue\n",
    "            else:\n",
    "                cur_possible_phones = cur_possible_phones + [[subword.upper()]]\n",
    "#                 print(i, j, subword, cur_possible_phones)\n",
    "\n",
    "            for cur_phones in cur_possible_phones:\n",
    "                extra_length = len(phones) - len(cur_phones)\n",
    "#                 print(\"EL\", extra_length)\n",
    "                if extra_length > 0:\n",
    "                    for a in range(0, extra_length + 1):\n",
    "                        subphones_end = len(phones) - (extra_length - a)\n",
    "                        subphones = phones[a:subphones_end]\n",
    "                        if len(subphones) != len(cur_phones):\n",
    "                            print(\"Lengths not equal....\")\n",
    "                        #print(subphones, cur_phones)\n",
    "                        if subphones == cur_phones:\n",
    "                            #print(\"MATCH\", subword, subphones)\n",
    "                            first = letterToPhone(word[0:j], phones[0:a])\n",
    "                            cur = letterToPhone(subword, subphones)\n",
    "                            last = letterToPhone(word[i+j:], phones[subphones_end:])\n",
    "                            # print(\"LTP Complete\", first, cur, last)\n",
    "                            return first + cur + last\n",
    "                    \n",
    "    return [[word.upper(), phones]]\n",
    "\n",
    "for segment in result_aligned[\"word_segments\"]:\n",
    "    word = segment[\"text\"]\n",
    "# for word in test_words:\n",
    "#     print(word)\n",
    "#     print(letterToPhone(word, wordbreak(word)[0]))\n",
    "#     print([word, wordbreak(word)[0]], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "c9907b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_words = []\n",
    "for segment in result_aligned[\"word_segments\"]:\n",
    "    word = segment[\"text\"]\n",
    "    chunked_words.append(letterToPhone(word, wordbreak(word)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "985f8669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['G', ['G']], ['OO', ['UH']], ['D', ['D']]]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_words[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4581c3d1",
   "metadata": {},
   "source": [
    "# Parsing out the transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "5b5bcf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = [o['text'] for o in result_aligned[\"word_segments\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "65270bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = '|'.join(ts).upper()\n",
    "ts = ts.replace(',', '')\n",
    "ts = ts.replace('.', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "81f26100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GOOD|EVENING|EVERYONE|OBAMA|HERE|BRINGING|YOU|THE|LATEST|NEWS|FROM|THE|UPSIDE-DOWN|WORLD'"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0655d36",
   "metadata": {},
   "source": [
    "# FORCED ALIGNMENT FROM WAV2VEC2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "25491e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams[\"figure.figsize\"] = [16.0, 4.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "bf7a366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model we are loading\n",
    "MODEL_NAME = 'WAV2VEC2_ASR_BASE_LV60K_960H'\n",
    "MODEL_NAME = 'WAV2VEC2_ASR_BASE_960H'\n",
    "# sampling rate the model expects\n",
    "# NOTE: most wav2vec models assume 16k sampling rate\n",
    "MODEL_SR = 16_000\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Point:\n",
    "    token_index: int\n",
    "    time_index: int\n",
    "    score: float\n",
    "\n",
    "\n",
    "# Merge the labels\n",
    "@dataclass\n",
    "class Segment:\n",
    "    label: str\n",
    "    start: int\n",
    "    end: int\n",
    "    score: float\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.label}\\t({self.score:4.2f}): [{self.start:5f}, {self.end:5f})\"\n",
    "\n",
    "    @property\n",
    "    def length(self):\n",
    "        return self.end - self.start\n",
    "\n",
    "\n",
    "\n",
    "def load_audio(wav_file, model_sr=MODEL_SR):\n",
    "    '''Loads `wav_file` using torchaudio.\n",
    "    \n",
    "    Resamples the audio to `MODEL_SR` if needed.\n",
    "    '''\n",
    "    wav, sr = torchaudio.load(wav_file)\n",
    "    if sr != model_sr:\n",
    "        print(f'Resampling from {sr} to {MODEL_SR}')\n",
    "        wav = torchaudio.transforms.Resample(sr, MODEL_SR)(wav)\n",
    "    return wav\n",
    "\n",
    "\n",
    "def clean_text(o):\n",
    "    clean = o.replace(',', ' ')\n",
    "    clean = clean.strip('\\n')\n",
    "    return clean\n",
    "\n",
    "def load_transcript(text_file):\n",
    "    '''Loads a transcript in `text_file`.\n",
    "    \n",
    "    Assumes one transcription per line.\n",
    "    '''\n",
    "    # load text file\n",
    "    lines = open(text_file, encoding=\"utf8\").readlines()\n",
    "    # cleanup the text\n",
    "    lines = [clean_text(line) for line in lines]\n",
    "    # replace spaces with special token `|`\n",
    "    lines = ['|'.join(line.split(' ')) for line in lines]\n",
    "    # make all characters upper-case for wav2vec token outputs\n",
    "    lines = [line.upper() for line in lines]\n",
    "    return lines\n",
    "    \n",
    "    \n",
    "def load_model(model_name):\n",
    "    '''Loads a pytorch `model_name` from torchaudio.\n",
    "    '''\n",
    "    bundle = getattr(torchaudio.pipelines, model_name, None)\n",
    "    if bundle:\n",
    "        model = bundle.get_model()\n",
    "    else:\n",
    "        raise ValueError(f'Could not find model: {model_name}')\n",
    "    return model, bundle\n",
    "\n",
    "\n",
    "\n",
    "def get_emissions(model, audio):\n",
    "    '''Gets token probabilities from `model` for the speech given in `audio`.\n",
    "    '''\n",
    "    emissions, _ = model(audio)\n",
    "    emissions = torch.log_softmax(emissions, dim=-1)\n",
    "    return emissions\n",
    "\n",
    "\n",
    "def get_trellis(emission, tokens, blank_id=0):\n",
    "    num_frame = emission.size(0)\n",
    "    num_tokens = len(tokens)\n",
    "\n",
    "    # Trellis has extra diemsions for both time axis and tokens.\n",
    "    # The extra dim for tokens represents <SoS> (start-of-sentence)\n",
    "    # The extra dim for time axis is for simplification of the code.\n",
    "    trellis = torch.empty((num_frame + 1, num_tokens + 1))\n",
    "    trellis[0, 0] = 0\n",
    "    trellis[1:, 0] = torch.cumsum(emission[:, 0], 0)\n",
    "    trellis[0, -num_tokens:] = -float(\"inf\")\n",
    "    trellis[-num_tokens:, 0] = float(\"inf\")\n",
    "\n",
    "    for t in range(num_frame):\n",
    "        trellis[t + 1, 1:] = torch.maximum(\n",
    "            # Score for staying at the same token\n",
    "            trellis[t, 1:] + emission[t, blank_id],\n",
    "            # Score for changing to the next token\n",
    "            trellis[t, :-1] + emission[t, tokens],\n",
    "        )\n",
    "    return trellis\n",
    "\n",
    "\n",
    "def backtrack(trellis, emission, tokens, blank_id=0):\n",
    "    # Note:\n",
    "    # j and t are indices for trellis, which has extra dimensions\n",
    "    # for time and tokens at the beginning.\n",
    "    # When referring to time frame index `T` in trellis,\n",
    "    # the corresponding index in emission is `T-1`.\n",
    "    # Similarly, when referring to token index `J` in trellis,\n",
    "    # the corresponding index in transcript is `J-1`.\n",
    "    j = trellis.size(1) - 1\n",
    "    t_start = torch.argmax(trellis[:, j]).item()\n",
    "\n",
    "    path = []\n",
    "    for t in range(t_start, 0, -1):\n",
    "        # 1. Figure out if the current position was stay or change\n",
    "        # Note (again):\n",
    "        # `emission[J-1]` is the emission at time frame `J` of trellis dimension.\n",
    "        # Score for token staying the same from time frame J-1 to T.\n",
    "        stayed = trellis[t - 1, j] + emission[t - 1, blank_id]\n",
    "        # Score for token changing from C-1 at T-1 to J at T.\n",
    "        changed = trellis[t - 1, j - 1] + emission[t - 1, tokens[j - 1]]\n",
    "\n",
    "        # 2. Store the path with frame-wise probability.\n",
    "        prob = emission[t - 1, tokens[j - 1] if changed > stayed else 0].exp().item()\n",
    "        # Return token index and time index in non-trellis coordinate.\n",
    "        path.append(Point(j - 1, t - 1, prob))\n",
    "\n",
    "        # 3. Update the token\n",
    "        if changed > stayed:\n",
    "            j -= 1\n",
    "            if j == 0:\n",
    "                break\n",
    "    else:\n",
    "        raise ValueError(\"Failed to align\")\n",
    "    return path[::-1]\n",
    "\n",
    "\n",
    "def merge_repeats(path, transcript):\n",
    "    i1, i2 = 0, 0\n",
    "    segments = []\n",
    "    while i1 < len(path):\n",
    "        while i2 < len(path) and path[i1].token_index == path[i2].token_index:\n",
    "            i2 += 1\n",
    "        score = sum(path[k].score for k in range(i1, i2)) / (i2 - i1)\n",
    "        segments.append(\n",
    "            Segment(\n",
    "                transcript[path[i1].token_index],\n",
    "                path[i1].time_index,\n",
    "                path[i2 - 1].time_index + 1,\n",
    "                score,\n",
    "            )\n",
    "        )\n",
    "        i1 = i2\n",
    "    return segments\n",
    "\n",
    "\n",
    "# Merge words\n",
    "def merge_words(segments, separator=\"|\"):\n",
    "    words = []\n",
    "    i1, i2 = 0, 0\n",
    "    while i1 < len(segments):\n",
    "        if i2 >= len(segments) or segments[i2].label == separator:\n",
    "            if i1 != i2:\n",
    "                segs = segments[i1:i2]\n",
    "                word = \"\".join([seg.label for seg in segs])\n",
    "                score = sum(seg.score * seg.length for seg in segs) / sum(seg.length for seg in segs)\n",
    "                words.append(Segment(word, segments[i1].start, segments[i2 - 1].end, score))\n",
    "            i1 = i2 + 1\n",
    "            i2 = i1\n",
    "        else:\n",
    "            i2 += 1\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "a8b4f3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example run through\n",
    "\n",
    "\n",
    "def run_forced_alignment(\n",
    "    model_name,\n",
    "    speech_file,\n",
    "    text_file,\n",
    "    device=None,\n",
    "    \n",
    "):\n",
    "    \n",
    "    # set the hardware device\n",
    "    device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # load the model\n",
    "    model, bundle = load_model(model_name)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # get model labels\n",
    "    labels = bundle.get_labels()\n",
    "    dictionary = {c: i for i, c in enumerate(labels)}\n",
    "    print(f'Aligning wav2vec2 tokens:')\n",
    "    keys = [f'{k}: {v}' for k,v in dictionary.items()]\n",
    "    print('\\n'.join(keys))\n",
    "\n",
    "    # load the audio and transcript\n",
    "    audio = load_audio(speech_file)\n",
    "    audio = audio.to(device)\n",
    "    print(f'Audio shape: {audio.shape}')\n",
    "    \n",
    "    # read in the transcript\n",
    "    if os.path.isfile(text_file):\n",
    "        transcript = load_transcript(text_file)[0]\n",
    "    else:\n",
    "        print(f'Note: using \"{text_file}\" as the transcript')\n",
    "        transcript = text_file\n",
    "\n",
    "    # get the token probabilities\n",
    "    emissions = get_emissions(model, audio)\n",
    "    emissions = emissions[0].detach().cpu()\n",
    "\n",
    "    # turn the transcript into tokens\n",
    "    tokens = [dictionary[c] for c in transcript]\n",
    "\n",
    "    # populate the trellis\n",
    "    trellis = get_trellis(emissions, tokens)\n",
    "\n",
    "    # walkback to find the most likely trellis path\n",
    "    path = backtrack(trellis, emissions, tokens)\n",
    "\n",
    "    # merge the paths with repeated labels\n",
    "    segments = merge_repeats(path, transcript)\n",
    "\n",
    "    # merge the words based on the separator token '|'\n",
    "    word_segments = merge_words(segments)\n",
    "    \n",
    "    # cleanup the model\n",
    "    del model\n",
    "    model = None\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return {\n",
    "        'character_segs': segments,\n",
    "        'word_segs': word_segments,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b06d5a7",
   "metadata": {},
   "source": [
    "### Cleaning up some GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "3a8efe55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "88e6e499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model\n",
    "model = None\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect() \n",
    "\n",
    "del model_a\n",
    "model_a = None\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "f8bc5921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9cfc9f",
   "metadata": {},
   "source": [
    "### Calling the Force Alignment on the `kitten_full.wav` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "1afd276c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligning wav2vec2 tokens:\n",
      "-: 0\n",
      "|: 1\n",
      "E: 2\n",
      "T: 3\n",
      "A: 4\n",
      "O: 5\n",
      "N: 6\n",
      "I: 7\n",
      "H: 8\n",
      "S: 9\n",
      "R: 10\n",
      "D: 11\n",
      "L: 12\n",
      "U: 13\n",
      "M: 14\n",
      "W: 15\n",
      "C: 16\n",
      "F: 17\n",
      "G: 18\n",
      "Y: 19\n",
      "P: 20\n",
      "B: 21\n",
      "V: 22\n",
      "K: 23\n",
      "': 24\n",
      "X: 25\n",
      "J: 26\n",
      "Q: 27\n",
      "Z: 28\n",
      "Resampling from 24000 to 16000\n",
      "Audio shape: torch.Size([1, 116566])\n",
      "Note: using \"GOOD|EVENING|EVERYONE|OBAMA|HERE|BRINGING|YOU|THE|LATEST|NEWS|FROM|THE|UPSIDE-DOWN|WORLD\" as the transcript\n"
     ]
    }
   ],
   "source": [
    "text_file = ts\n",
    "model_name = MODEL_NAME\n",
    "\n",
    "\n",
    "segs = run_forced_alignment(\n",
    "    model_name,\n",
    "    audio_file_name,\n",
    "    text_file,\n",
    "    device='cpu',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "63fcdbbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[G\t(0.90): [52.000000, 54.000000),\n",
       " O\t(1.00): [54.000000, 57.000000),\n",
       " O\t(1.00): [57.000000, 58.000000),\n",
       " D\t(0.83): [58.000000, 60.000000),\n",
       " |\t(0.57): [60.000000, 64.000000),\n",
       " E\t(1.00): [64.000000, 65.000000),\n",
       " V\t(0.50): [65.000000, 67.000000),\n",
       " E\t(0.51): [67.000000, 69.000000),\n",
       " N\t(0.70): [69.000000, 73.000000),\n",
       " I\t(1.00): [73.000000, 74.000000),\n",
       " N\t(0.52): [74.000000, 76.000000),\n",
       " G\t(0.62): [76.000000, 78.000000),\n",
       " |\t(0.77): [78.000000, 82.000000),\n",
       " E\t(1.00): [82.000000, 83.000000),\n",
       " V\t(1.00): [83.000000, 85.000000),\n",
       " E\t(1.00): [85.000000, 86.000000),\n",
       " R\t(0.70): [86.000000, 88.000000),\n",
       " Y\t(0.93): [88.000000, 96.000000),\n",
       " O\t(1.00): [96.000000, 97.000000),\n",
       " N\t(0.99): [97.000000, 98.000000),\n",
       " E\t(0.37): [98.000000, 101.000000),\n",
       " |\t(0.98): [101.000000, 146.000000),\n",
       " O\t(0.91): [146.000000, 150.000000),\n",
       " B\t(1.00): [150.000000, 155.000000),\n",
       " A\t(0.99): [155.000000, 159.000000),\n",
       " M\t(0.99): [159.000000, 162.000000),\n",
       " A\t(0.75): [162.000000, 166.000000),\n",
       " |\t(0.94): [166.000000, 167.000000),\n",
       " H\t(1.00): [167.000000, 170.000000),\n",
       " E\t(0.96): [170.000000, 173.000000),\n",
       " R\t(1.00): [173.000000, 174.000000),\n",
       " E\t(0.57): [174.000000, 176.000000),\n",
       " |\t(0.67): [176.000000, 179.000000),\n",
       " B\t(1.00): [179.000000, 181.000000),\n",
       " R\t(1.00): [181.000000, 183.000000),\n",
       " I\t(1.00): [183.000000, 184.000000),\n",
       " N\t(0.98): [184.000000, 186.000000),\n",
       " G\t(1.00): [186.000000, 190.000000),\n",
       " I\t(0.99): [190.000000, 191.000000),\n",
       " N\t(0.99): [191.000000, 192.000000),\n",
       " G\t(0.95): [192.000000, 194.000000),\n",
       " |\t(0.57): [194.000000, 196.000000),\n",
       " Y\t(1.00): [196.000000, 198.000000),\n",
       " O\t(1.00): [198.000000, 199.000000),\n",
       " U\t(0.75): [199.000000, 203.000000),\n",
       " |\t(0.50): [203.000000, 205.000000),\n",
       " T\t(1.00): [205.000000, 206.000000),\n",
       " H\t(1.00): [206.000000, 207.000000),\n",
       " E\t(0.53): [207.000000, 209.000000),\n",
       " |\t(0.75): [209.000000, 211.000000),\n",
       " L\t(1.00): [211.000000, 214.000000),\n",
       " A\t(1.00): [214.000000, 217.000000),\n",
       " T\t(1.00): [217.000000, 221.000000),\n",
       " E\t(0.78): [221.000000, 223.000000),\n",
       " S\t(0.99): [223.000000, 225.000000),\n",
       " T\t(0.86): [225.000000, 227.000000),\n",
       " |\t(0.74): [227.000000, 231.000000),\n",
       " N\t(0.98): [231.000000, 236.000000),\n",
       " E\t(1.00): [236.000000, 237.000000),\n",
       " W\t(0.96): [237.000000, 242.000000),\n",
       " S\t(0.63): [242.000000, 244.000000),\n",
       " |\t(0.75): [244.000000, 248.000000),\n",
       " F\t(1.00): [248.000000, 250.000000),\n",
       " R\t(0.92): [250.000000, 252.000000),\n",
       " O\t(1.00): [252.000000, 253.000000),\n",
       " M\t(0.66): [253.000000, 256.000000),\n",
       " |\t(1.00): [256.000000, 257.000000),\n",
       " T\t(1.00): [257.000000, 258.000000),\n",
       " H\t(0.99): [258.000000, 260.000000),\n",
       " E\t(0.50): [260.000000, 262.000000),\n",
       " |\t(1.00): [262.000000, 266.000000),\n",
       " U\t(1.00): [266.000000, 268.000000),\n",
       " P\t(1.00): [268.000000, 273.000000),\n",
       " S\t(1.00): [273.000000, 281.000000),\n",
       " I\t(1.00): [281.000000, 283.000000),\n",
       " D\t(1.00): [283.000000, 285.000000),\n",
       " E\t(1.00): [285.000000, 286.000000),\n",
       " -\t(0.60): [286.000000, 291.000000),\n",
       " D\t(1.00): [291.000000, 297.000000),\n",
       " O\t(1.00): [297.000000, 298.000000),\n",
       " W\t(1.00): [298.000000, 301.000000),\n",
       " N\t(0.75): [301.000000, 305.000000),\n",
       " |\t(0.77): [305.000000, 309.000000),\n",
       " W\t(1.00): [309.000000, 313.000000),\n",
       " O\t(0.59): [313.000000, 315.000000),\n",
       " R\t(0.73): [315.000000, 319.000000),\n",
       " L\t(0.51): [319.000000, 321.000000),\n",
       " D\t(0.65): [321.000000, 322.000000)]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# character segmentation map\n",
    "segs['character_segs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "eecfa9f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['H', ['HH']], ['E', ['IY']], ['RE', ['R']]]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view a few chunked words\n",
    "chunked_words[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe437a7",
   "metadata": {},
   "source": [
    "# HERE: MAPPING CHARACTER DURATIONS TO PHONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "ae3a7134",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_segs = segs['character_segs'][:]\n",
    "char_ptr = 0\n",
    "\n",
    "\n",
    "def frame_to_ms(frame):\n",
    "    return int(frame * 2) # * 20\n",
    "\n",
    "# stores the info for the needed transcriptions\n",
    "full_info = []\n",
    "\n",
    "# step through the chunked, phoneme words\n",
    "for cword in chunked_words:\n",
    "    \n",
    "    # cleanup punctuation from the word\n",
    "    word = ''.join(o[0] for o in cword).lower()\n",
    "    word = word.replace(',', '')\n",
    "    word = word.replace('.', '')\n",
    "        \n",
    "    # alignments for this word\n",
    "    alns = []\n",
    "    \n",
    "    # step through the pairs of letters/phones for this word\n",
    "    for (letters, phones) in cword:\n",
    "        \n",
    "        # cleanup punctuation from the letters\n",
    "        if letters:\n",
    "            letters = letters.replace(',', '')\n",
    "            letters = letters.replace('.', '')\n",
    "            \n",
    "        # group the character segments for this letter/phone pair\n",
    "        cur_segs = []\n",
    "        while ''.join(o.label for o in cur_segs) != letters and char_ptr < len(char_segs):\n",
    "            cur_segs.append(char_segs[char_ptr])\n",
    "            char_ptr += 1\n",
    "            \n",
    "        # TODO: handle cases when:\n",
    "        ## A) There is no phone for the letters\n",
    "        ## B) There are more than on phones for the letter\n",
    "        num_phones = len(phones)\n",
    "        if phones:\n",
    "            \n",
    "            # start and stop time of the entire segments\n",
    "            if cur_segs:\n",
    "                beg_ms = cur_beg = frame_to_ms(cur_segs[0].start)\n",
    "                end_ms = frame_to_ms(cur_segs[-1].end)\n",
    "\n",
    "                dur = end_ms - beg_ms\n",
    "                dur_per_phone = dur / num_phones\n",
    "\n",
    "                for idx,phone in enumerate(phones):\n",
    "                    cur_end = round(cur_beg + dur_per_phone)\n",
    "                    if cur_segs:\n",
    "                        alns.append({\n",
    "                            'ph': phone,\n",
    "                            'bg': cur_beg,\n",
    "                            'ed': cur_end,\n",
    "                        })\n",
    "                    cur_beg = cur_end\n",
    "                \n",
    "        # if no phones are given, then extend the previous phone's durations   \n",
    "        else:\n",
    "            if alns and cur_segs:\n",
    "                prev_seg = alns[-1]\n",
    "                prev_seg['ed'] = frame_to_ms(cur_segs[-1].end)\n",
    "                alns[-1] = prev_seg\n",
    "                \n",
    "        \n",
    "    if len(full_info) == 0:\n",
    "        full_info.append({\n",
    "            'word': 'sil',\n",
    "            'phones': [{\n",
    "                'ph': 'SIL',\n",
    "                'bg': 0,\n",
    "                'ed': alns[0][\"bg\"],\n",
    "            }]\n",
    "        })\n",
    "\n",
    "    # add the info we need for the video generation\n",
    "    info = {'word': word, 'phones': alns}\n",
    "    full_info.append(info)\n",
    "            \n",
    "    # check if we still have segments to process\n",
    "    if char_ptr < len(char_segs):\n",
    "        \n",
    "        # if we're at the end of a word, add a silence\n",
    "        if char_segs[char_ptr].label == '|':\n",
    "            char_ptr += 1\n",
    "            full_info.append({\n",
    "                'word': 'sil',\n",
    "                'phones': [{\n",
    "                    'ph': 'SIL',\n",
    "                    'bg': full_info[-1]['phones'][-1]['ed'],\n",
    "                    'ed': frame_to_ms(char_segs[char_ptr].start),\n",
    "                }]\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "2b2a956a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'sil', 'phones': [{'ph': 'SIL', 'bg': 0, 'ed': 104}]},\n",
       " {'word': 'good',\n",
       "  'phones': [{'ph': 'G', 'bg': 104, 'ed': 108},\n",
       "   {'ph': 'UH', 'bg': 108, 'ed': 116},\n",
       "   {'ph': 'D', 'bg': 116, 'ed': 120}]},\n",
       " {'word': 'sil', 'phones': [{'ph': 'SIL', 'bg': 120, 'ed': 128}]},\n",
       " {'word': 'evening',\n",
       "  'phones': [{'ph': 'IY', 'bg': 128, 'ed': 130},\n",
       "   {'ph': 'V', 'bg': 130, 'ed': 138},\n",
       "   {'ph': 'N', 'bg': 138, 'ed': 146},\n",
       "   {'ph': 'IH', 'bg': 146, 'ed': 148},\n",
       "   {'ph': 'NG', 'bg': 148, 'ed': 156}]},\n",
       " {'word': 'sil', 'phones': [{'ph': 'SIL', 'bg': 156, 'ed': 164}]},\n",
       " {'word': 'everyone',\n",
       "  'phones': [{'ph': 'EH', 'bg': 164, 'ed': 166},\n",
       "   {'ph': 'V', 'bg': 166, 'ed': 170},\n",
       "   {'ph': 'IY', 'bg': 170, 'ed': 192},\n",
       "   {'ph': 'W', 'bg': 192, 'ed': 193},\n",
       "   {'ph': 'AH', 'bg': 193, 'ed': 194},\n",
       "   {'ph': 'N', 'bg': 194, 'ed': 202}]},\n",
       " {'word': 'sil', 'phones': [{'ph': 'SIL', 'bg': 202, 'ed': 292}]},\n",
       " {'word': 'obama',\n",
       "  'phones': [{'ph': 'OW', 'bg': 292, 'ed': 300},\n",
       "   {'ph': 'B', 'bg': 300, 'ed': 310},\n",
       "   {'ph': 'AH', 'bg': 310, 'ed': 332}]},\n",
       " {'word': 'sil', 'phones': [{'ph': 'SIL', 'bg': 332, 'ed': 334}]},\n",
       " {'word': 'here',\n",
       "  'phones': [{'ph': 'HH', 'bg': 334, 'ed': 340},\n",
       "   {'ph': 'IY', 'bg': 340, 'ed': 346},\n",
       "   {'ph': 'R', 'bg': 346, 'ed': 352}]},\n",
       " {'word': 'sil', 'phones': [{'ph': 'SIL', 'bg': 352, 'ed': 358}]},\n",
       " {'word': 'bringing',\n",
       "  'phones': [{'ph': 'B', 'bg': 358, 'ed': 362},\n",
       "   {'ph': 'R', 'bg': 362, 'ed': 366},\n",
       "   {'ph': 'IH', 'bg': 366, 'ed': 368},\n",
       "   {'ph': 'NG', 'bg': 368, 'ed': 380},\n",
       "   {'ph': 'IH', 'bg': 380, 'ed': 382},\n",
       "   {'ph': 'NG', 'bg': 382, 'ed': 388}]},\n",
       " {'word': 'sil', 'phones': [{'ph': 'SIL', 'bg': 388, 'ed': 392}]},\n",
       " {'word': 'you',\n",
       "  'phones': [{'ph': 'Y', 'bg': 392, 'ed': 396},\n",
       "   {'ph': 'UW', 'bg': 396, 'ed': 406}]},\n",
       " {'word': 'sil', 'phones': [{'ph': 'SIL', 'bg': 406, 'ed': 410}]},\n",
       " {'word': 'the',\n",
       "  'phones': [{'ph': 'DH', 'bg': 410, 'ed': 414},\n",
       "   {'ph': 'AH', 'bg': 414, 'ed': 418}]},\n",
       " {'word': 'sil', 'phones': [{'ph': 'SIL', 'bg': 418, 'ed': 422}]},\n",
       " {'word': 'latest',\n",
       "  'phones': [{'ph': 'L', 'bg': 422, 'ed': 428},\n",
       "   {'ph': 'EY', 'bg': 428, 'ed': 434},\n",
       "   {'ph': 'T', 'bg': 434, 'ed': 446},\n",
       "   {'ph': 'S', 'bg': 446, 'ed': 450},\n",
       "   {'ph': 'T', 'bg': 450, 'ed': 454}]},\n",
       " {'word': 'sil', 'phones': [{'ph': 'SIL', 'bg': 454, 'ed': 462}]},\n",
       " {'word': 'news',\n",
       "  'phones': [{'ph': 'N', 'bg': 462, 'ed': 472},\n",
       "   {'ph': 'UW', 'bg': 472, 'ed': 484},\n",
       "   {'ph': 'Z', 'bg': 484, 'ed': 488}]},\n",
       " {'word': 'sil', 'phones': [{'ph': 'SIL', 'bg': 488, 'ed': 496}]},\n",
       " {'word': 'from',\n",
       "  'phones': [{'ph': 'F', 'bg': 496, 'ed': 500},\n",
       "   {'ph': 'R', 'bg': 500, 'ed': 504},\n",
       "   {'ph': 'AH', 'bg': 504, 'ed': 506},\n",
       "   {'ph': 'M', 'bg': 506, 'ed': 512}]},\n",
       " {'word': 'sil', 'phones': [{'ph': 'SIL', 'bg': 512, 'ed': 514}]},\n",
       " {'word': 'the',\n",
       "  'phones': [{'ph': 'DH', 'bg': 514, 'ed': 519},\n",
       "   {'ph': 'AH', 'bg': 519, 'ed': 524}]},\n",
       " {'word': 'sil', 'phones': [{'ph': 'SIL', 'bg': 524, 'ed': 532}]},\n",
       " {'word': 'upside-down',\n",
       "  'phones': [{'ph': 'AH', 'bg': 532, 'ed': 536},\n",
       "   {'ph': 'P', 'bg': 536, 'ed': 546},\n",
       "   {'ph': 'S', 'bg': 546, 'ed': 562},\n",
       "   {'ph': 'AY', 'bg': 562, 'ed': 566},\n",
       "   {'ph': 'D', 'bg': 566, 'ed': 582},\n",
       "   {'ph': 'D', 'bg': 582, 'ed': 594},\n",
       "   {'ph': 'AW', 'bg': 594, 'ed': 602},\n",
       "   {'ph': 'N', 'bg': 602, 'ed': 610}]},\n",
       " {'word': 'sil', 'phones': [{'ph': 'SIL', 'bg': 610, 'ed': 618}]},\n",
       " {'word': 'world',\n",
       "  'phones': [{'ph': 'W', 'bg': 618, 'ed': 626},\n",
       "   {'ph': 'ER', 'bg': 626, 'ed': 638},\n",
       "   {'ph': 'L', 'bg': 638, 'ed': 642},\n",
       "   {'ph': 'D', 'bg': 642, 'ed': 644}]}]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "02a64463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to  ../AAAI22-one-shot-talking-face/samples/phonemes/obama_intro.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/jfuentes/Projects/infinitenews/ForcedAlignment'"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_name = f\"../AAAI22-one-shot-talking-face/samples/phonemes/{base_audio_file_name}.json\"\n",
    "print(\"Writing to \", file_name)\n",
    "with open(file_name, \"w\") as f1:\n",
    "    json.dump(full_info, f1)\n",
    "base_audio_file_name\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abcbff7",
   "metadata": {},
   "source": [
    "### Group into needed format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ee2e7cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "150922fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O\t(1.00): [3944.000000, 3945.000000)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segs['character_segs'][-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d6ffeb82",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'phone_align' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mphone_align\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'phone_align' is not defined"
     ]
    }
   ],
   "source": [
    "phone_align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9742bc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49a9585",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(o.start / 50, o.end / 50) for o in segs['word_segs']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6cb23d",
   "metadata": {},
   "source": [
    "# Comparing the alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a37f7788",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_aligned[\"word_segments\"];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625028b9",
   "metadata": {},
   "source": [
    "### NODE: older code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b6ad6670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'sil', 'phones': [{'ph': 'SIL', 'bg': 0, 'ed': 10}]},\n",
       " {'word': 'Good',\n",
       "  'phones': [{'ph': 'G', 'bg': 10, 'ed': 14},\n",
       "   {'ph': 'UH', 'bg': 14, 'ed': 18},\n",
       "   {'ph': 'D', 'bg': 18, 'ed': 22}]},\n",
       " {'word': 'sil', 'phones': [{'ph': 'SIL', 'bg': 22, 'ed': 30}]},\n",
       " {'word': 'evening,',\n",
       "  'phones': [{'ph': 'IY', 'bg': 30, 'ed': 34},\n",
       "   {'ph': 'V', 'bg': 34, 'ed': 38},\n",
       "   {'ph': 'N', 'bg': 38, 'ed': 42},\n",
       "   {'ph': 'IH', 'bg': 42, 'ed': 46},\n",
       "   {'ph': 'NG', 'bg': 46, 'ed': 50}]},\n",
       " {'word': 'sil', 'phones': [{'ph': 'SIL', 'bg': 50, 'ed': 60}]}]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# {'text': 'Good', 'start': 0.10120481927710842, 'end': 0.32385542168674697},\n",
    "# INTO\n",
    "# [\n",
    "#     {\n",
    "#         \"word\": \"sil\",\n",
    "#         \"phones\": [\n",
    "#             {\n",
    "#                 \"ph\": \"SIL\",\n",
    "#                 \"bg\": 0,\n",
    "#                 \"ed\": 126\n",
    "#             }\n",
    "#         ]\n",
    "#     },\n",
    "#     {\n",
    "#         \"word\": \"YOU\",\n",
    "#         \"phones\": [\n",
    "#             {\n",
    "#                 \"ph\": \"Y\",\n",
    "#                 \"bg\": 126,\n",
    "#                 \"ed\": 138\n",
    "#             },\n",
    "#             {\n",
    "#                 \"ph\": \"UW\",\n",
    "#                 \"bg\": 138,\n",
    "#                 \"ed\": 141\n",
    "#             }\n",
    "#         ]\n",
    "#     },\n",
    "from math import trunc\n",
    "\n",
    "def makeSegment(word, phones, start, end):\n",
    "    total = end - start\n",
    "    time_per_phone = trunc(total / len(phones))\n",
    "#     print(total, time_per_phone)\n",
    "    expanded_phones = [{\"ph\": phone, \"bg\": start + (i * time_per_phone), \"ed\": start + ((i + 1) * time_per_phone)} for i, phone in enumerate(phones)]\n",
    "    return {\"word\": word, \"phones\": expanded_phones}\n",
    "\n",
    "def makeSil(start, end):\n",
    "    return {\"word\": \"sil\", \"phones\": [{\"ph\": \"SIL\", \"bg\": start, \"ed\": end}]}\n",
    "  \n",
    "all_phonemes = []\n",
    "for segment in result_aligned[\"word_segments\"]:\n",
    "    word = segment[\"text\"]\n",
    "    #Get rid of numbers in phones\n",
    "    phonemes = wordbreak(word)[0]\n",
    "    #print(phonemes)\n",
    "    start_time = trunc(segment[\"start\"] * 100)\n",
    "    end_time = trunc(segment[\"end\"] * 100)\n",
    "    \n",
    "    if len(all_phonemes) > 0:\n",
    "        last = all_phonemes[-1]\n",
    "        last_end = last[\"phones\"][-1][\"ed\"]\n",
    "        if start_time - last_end > 5:\n",
    "            sil = makeSil(last_end, start_time)\n",
    "            all_phonemes.append(sil)\n",
    "        else:\n",
    "            start_time = last_end\n",
    "    else:\n",
    "        sil = makeSil(0, start_time)\n",
    "        all_phonemes.append(sil)\n",
    "        \n",
    "    segment = makeSegment(word, phonemes, start_time, end_time)\n",
    "    all_phonemes.append(segment)\n",
    "\n",
    "all_phonemes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "37dd754f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to  ../AAAI22-one-shot-talking-face/samples/phonemes/kitten_full_og.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_name = \"../AAAI22-one-shot-talking-face/samples/phonemes/{name}_og.json\".format(name=base_audio_file_name)\n",
    "print(\"Writing to \", file_name)\n",
    "f1 = open(file_name, 'w')\n",
    "json.dump(all_phonemes, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4326767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb2d9d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de9ac94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f6aad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0c59c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64db7e3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce09a563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c7e085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43f5154",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90109a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
